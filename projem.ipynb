{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decf3d20",
   "metadata": {},
   "source": [
    "# BridgeMind: Agentic Hybrid RAG,\n",
    "Bu bölüm, sisteminin çalışması için gerekli temel kütüphaneleri yükler. \n",
    "- Sistem, hem yerel cihazlarda hem de **Google Colab** üzerinde sorunsuz çalışacak şekilde yapılandırılmıştır.\n",
    "- `LangChain`, `ChromaDB` ve `LlamaCpp`gibi kritik bileşenler burada tanımlanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# --- STEP 1: COLAB ENVIRONMENT SETUP & REPO CLONING ---\n",
    "# This block ensures that when the professor opens the link, files are pulled automatically.\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Colab environment detected. Cloning repository and installing dependencies...\")\n",
    "    \n",
    "    # Clone the repo to get PDFs and other files into Colab's local storage\n",
    "    !git clone https://github.com/serifeeroglu/BridgeMind-RAG.git .\n",
    "    \n",
    "    # Install required libraries specifically for Colab environment\n",
    "    !pip install -q duckduckgo-search langchain-community langchain-huggingface chromadb llama-cpp-python pypdf sentence-transformers\n",
    "    \n",
    "    FOLDER_PATH = \"/content\"\n",
    "else:\n",
    "    # Local path for VS Code environment\n",
    "    FOLDER_PATH = r\"C:\\Users\\MSI\\OneDrive\\Masaüstü\\ragproje\"\n",
    "\n",
    "# --- STEP 2: LIBRARY IMPORTS ---\n",
    "try:\n",
    "    from duckduckgo_search import DDGS\n",
    "except ImportError:\n",
    "    print(\"ERROR: 'duckduckgo-search' library not found.\")\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# --- STEP 3: SYSTEM CONFIGURATIONS & LOGGING ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# Initialize performance stats for the final report\n",
    "perf_stats = {\"total_queries\": 0, \"pdf_hits\": 0, \"web_hits\": 0, \"total_time\": 0}\n",
    "\n",
    "print(f\"System Ready. Working Directory: {FOLDER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849382ff",
   "metadata": {},
   "source": [
    "## 2. Dinamik Yol Yapılandırması ve Otomatik Model Yükleyici\n",
    "Projenin farklı ortamlarda (Colab/Yerel) hatasız çalışması için:\n",
    "- Kod, çalışma ortamını otomatik olarak algılar ve dosya yollarını buna göre ayarlar.\n",
    "- Eğer **Phi-3-mini GGUF** modeli mevcut değilse, Hugging Face üzerinden otomatik olarak indirilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 2: Path Configuration & Automatic Model Download \n",
    "# Dynamic path selection: Uses local path for VS Code, root for Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    FOLDER_PATH = \"/content/\"\n",
    "else:\n",
    "    FOLDER_PATH = r\"C:\\Users\\MSI\\OneDrive\\Masaüstü\\ragproje\"\n",
    "\n",
    "MODEL_PATH = os.path.join(FOLDER_PATH, \"Phi-3-mini-4k-instruct-q4.gguf\")\n",
    "\n",
    "# Automated model downloader (Ensures the model exists for the evaluator)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Model file not found. Downloading Phi-3-mini (approx. 2.2GB)...\")\n",
    "    import requests\n",
    "    url = \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(MODEL_PATH, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(\"Model downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90844fea",
   "metadata": {},
   "source": [
    "## 3. Harici Bilgi Kaynağı: Web Arama Aracı\n",
    "Sistemin 'Ajan' (Agentic) özelliğini sağlayan bu araç, yerel PDF dökümanlarında bulunmayan bilgileri getirmekle görevlidir.\n",
    "- Eğer yerel veritabanı sorguya cevap veremezse, sistem otonom olarak **DuckDuckGo API** üzerinden güncel web verilerini tarar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 3: Web Search Tool \n",
    "def web_search_tool(query):\n",
    "    print(f\"\\n AGENT: Knowledge not found in local PDFs. Searching the web for: {query}\")\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            search_query = f\"{query} detailed explanation and steps\"\n",
    "            results = []\n",
    "            raw_results = ddgs.text(search_query, max_results=5)\n",
    "            for r in raw_results:\n",
    "                results.append(f\"Source: {r['title']}\\nSnippet: {r['body']}\")\n",
    "            return \"\\n\\n\".join(results)\n",
    "    except Exception as e:\n",
    "        return f\"Web search failed: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11490d42",
   "metadata": {},
   "source": [
    "## 4. Akademik PDF İndeksleme ve Vektör Veritabanı (RAG)\n",
    "Bu aşamada yüklenen akademik makaleler işlenerek sorgulanabilir hale getirilir:\n",
    "- **Recursive Character Text Splitting:** Metinler, anlamsal bütünlüğü korumak adına 800 karakterlik parçalara bölünür.\n",
    "- **HuggingFace Embeddings:** Cümleler vektörel temsillere dönüştürülür ve hızlı benzerlik araması için **ChromaDB** içerisinde saklanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866d99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING SYSTEM & INDEXING LOCAL DOCUMENTS...\n",
      "SUCCESS: 10 PDFs indexed into 1441 chunks.\n"
     ]
    }
   ],
   "source": [
    "#  STEP 4: Vector Database (PDF Indexing) \n",
    "print(\"INITIALIZING SYSTEM & INDEXING LOCAL DOCUMENTS...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(FOLDER_PATH):\n",
    "    print(f\"ERROR: Directory {FOLDER_PATH} not found!\")\n",
    "else:\n",
    "    pdf_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        print(\"WARNING: No PDF files found! Please ensure PDFs are in the folder.\")\n",
    "    else:\n",
    "        all_docs = []\n",
    "        for pdf in pdf_files:\n",
    "            loader = PyPDFLoader(os.path.join(FOLDER_PATH, pdf))\n",
    "            all_docs.extend(loader.load())\n",
    "\n",
    "        # Optimized chunking for academic papers\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "        texts = text_splitter.split_documents(all_docs)\n",
    "        \n",
    "        # Build Vector Store\n",
    "        vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
    "        print(f\"SUCCESS: {len(pdf_files)} PDFs indexed into {len(texts)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a23c5",
   "metadata": {},
   "source": [
    "## 5. Ajan Mantığı ve Karar Mekanizması (Prompt Engineering)\n",
    "Sistemin karar verici katmanı burada devreye girer:\n",
    "- **Phi-3-mini** modeli, `temperature=0.0` ayarı ile en yüksek doğrulukta çalışacak şekilde yapılandırılmıştır.\n",
    "- **Strict Evaluator:** Model, sorulan soruyu önce dökümanlarda arar; eğer bilgi dökümanda yoksa uydurmak yerine `SEARCH_WEB` komutunu döndürür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d3717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from C:\\Users\\MSI\\OneDrive\\Masaüstü\\ragproje\\Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: printing all EOG tokens:\n",
      "load:   - 32000 ('<|endoftext|>')\n",
      "load:   - 32007 ('<|end|>')\n",
      "load: special tokens cache size = 67\n",
      "load: token to piece cache size = 0.1690 MB\n",
      "print_info: arch             = phi3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 96\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 96\n",
      "print_info: n_embd_head_v    = 96\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 3072\n",
      "print_info: n_embd_v_gqa     = 3072\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.82 B\n",
      "print_info: general.name     = Phi3\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32064\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32007 '<|end|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32000 '<|endoftext|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: EOG token        = 32007 '<|end|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 114 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  1242.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...........................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 64\n",
      "llama_context: n_ubatch      = 8\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1536.00 MiB\n",
      "llama_kv_cache_unified: size = 1536.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 1560\n",
      "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "llama_context:        CPU compute buffer size =     5.38 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi3', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'phi3.embedding_length': '3072', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '15', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "#  STEP 5: LLM Initialization & Strict Agent Logic \n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=4096,\n",
    "    max_tokens=1024, \n",
    "    temperature=0.0, # Set to 0 for factual accuracy\n",
    "    n_gpu_layers=-1 # Use GPU acceleration\n",
    ")\n",
    "\n",
    "def ask_agentic_rag(query):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Retrieve top 4 relevant context snippets\n",
    "    docs = vectorstore.similarity_search(query, k=4)\n",
    "    context_text = \"\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    # Strict prompt to force a binary decision: Answer or Search Web\n",
    "    decision_prompt = f\"\"\"<|system|> \n",
    "    You are a strict academic researcher. \n",
    "    1. Use ONLY the Context below to answer. \n",
    "    2. If the answer is NOT in the Context, reply ONLY with the word: SEARCH_WEB\n",
    "    3. Do NOT mention the web or search in your initial check.\n",
    "    <|end|>\n",
    "    <|user|> Context: {context_text} \\n Question: {query} <|end|>\n",
    "    <|assistant|>\"\"\"\n",
    "    \n",
    "    decision = llm.invoke(decision_prompt).strip()\n",
    "    \n",
    "    # Logic to switch between Local PDF and Web Search\n",
    "    if \"SEARCH_WEB\" in decision.upper() or not decision:\n",
    "        web_info = web_search_tool(query)\n",
    "        final_prompt = f\"<|system|> Summarize web results. <|end|> <|user|> Web: {web_info} \\n Q: {query} <|end|> <|assistant|>\"\n",
    "        response = llm.invoke(final_prompt).strip()\n",
    "        source = \"Web Search (Live Data)\"\n",
    "        perf_stats[\"web_hits\"] += 1\n",
    "    else:\n",
    "        response = decision\n",
    "        source = \"Local Academic PDFs\"\n",
    "        perf_stats[\"pdf_hits\"] += 1\n",
    "    \n",
    "    perf_stats[\"total_time\"] += (time.time() - start_time)\n",
    "    perf_stats[\"total_queries\"] += 1\n",
    "    return response, source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958fb37",
   "metadata": {},
   "source": [
    "## 6. Sorgu Döngüsü ve Performans Analizi\n",
    "Sistemin etkileşimli çalışma alanıdır:\n",
    "- Kullanıcıdan gelen soruları alır ve cevabın kaynağını (**Yerel PDF** veya **Web**) belirtir.\n",
    "- Döngü sonunda; toplam sorgu sayısı, PDF/Web isabet oranları ve ortalama gecikme süresi raporlanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79604a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENTIC RAG SYSTEM READY (LOCAL + WEB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   16934.47 ms\n",
      "llama_perf_context_print: prompt eval time =   16934.38 ms /   590 tokens (   28.70 ms per token,    34.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16266.64 ms /   225 runs   (   72.30 ms per token,    13.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   33290.68 ms /   815 tokens\n",
      "llama_perf_context_print:    graphs reused =        271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANALYSIS 1 \n",
      " ANSWER: In the RAG framework, the 'retriever' and 'generator' play specific roles in processing user queries and generating responses. The retriever component (pη(z|x)) is responsible for returning distributions over text passages given a query x. It uses parameters η to retrieve relevant information from external data sources. This retrieved information populates the prompt template, which is then passed to the generator component.\n",
      "\n",
      "The 'generator' component (pθ(yi|x,z,y1:i−1)) takes in the query x, the retrieved passages z, and previous generated sequences y1:i-1 as input parameters. It parametrizes a model that generates the target sequence y based on this information. The generator uses the context provided by the retriever to produce an appropriate response to the user's query.\n",
      "\n",
      "In summary, the 'retriever' gathers relevant data from external sources and populates the prompt template, while the 'generator' creates a coherent and accurate response using this information along with previous generated sequences. \n",
      " SOURCE: Local Academic PDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 29 prefix-match hit, remaining 425 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16934.47 ms\n",
      "llama_perf_context_print: prompt eval time =   12106.41 ms /   425 tokens (   28.49 ms per token,    35.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12647.87 ms /   183 runs   (   69.11 ms per token,    14.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   24824.05 ms /   608 tokens\n",
      "llama_perf_context_print:    graphs reused =        217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANALYSIS 2 \n",
      " ANSWER: GraphRAG uses community summaries by recursively creating increasingly global summaries. It does this by using the LLM to create summaries spanning a hierarchy of nested modular communities of closely related nodes, which are partitioned into these communities based on their inherent modularity. Specifically, it generates community summaries by adding various element summaries (for nodes, edges, and related claims) to a community summary template. Community summaries from lower-level communities are used to generate summaries for higher-level communities in the following way: leaf-level communities have their element summaries prioritized and iteratively added to the LLM context window until the token limit is reached. This approach allows GraphRAG to create more comprehensive and globally relevant summaries compared to standard RAG methods, which may not utilize community hierarchies or modularity in the same way. \n",
      " SOURCE: Local Academic PDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 29 prefix-match hit, remaining 290 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16934.47 ms\n",
      "llama_perf_context_print: prompt eval time =    8312.44 ms /   290 tokens (   28.66 ms per token,    34.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     411.20 ms /     6 runs   (   68.53 ms per token,    14.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    8725.95 ms /   296 tokens\n",
      "llama_perf_context_print:    graphs reused =         31\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_21228\\1398812118.py:5: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " AGENT: Knowledge not found in local PDFs. Searching the web for: What is the current price of Bitcoin in USD right now?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 731 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16934.47 ms\n",
      "llama_perf_context_print: prompt eval time =   21412.22 ms /   731 tokens (   29.29 ms per token,    34.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4473.58 ms /    67 runs   (   66.77 ms per token,    14.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   25906.83 ms /   798 tokens\n",
      "llama_perf_context_print:    graphs reused =        132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANALYSIS 3 \n",
      " ANSWER: I'm unable to provide real-time data. However, you can check the current price of Bitcoin in USD by visiting financial news websites like CoinDesk or CNBC, using cryptocurrency market tracking apps, or checking reputable financial news sources online for up-to-date information. \n",
      " SOURCE: Web Search (Live Data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 521 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16934.47 ms\n",
      "llama_perf_context_print: prompt eval time =   14839.71 ms /   521 tokens (   28.48 ms per token,    35.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7832.27 ms /   117 runs   (   66.94 ms per token,    14.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   22712.08 ms /   638 tokens\n",
      "llama_perf_context_print:    graphs reused =        162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANALYSIS 4 \n",
      " ANSWER: In Self-RAG, 'critique tokens' are special reflection tokens that help the model generate and evaluate its own text generation. These tokens enable the LM to predict not only the next token from its original vocabulary but also critique or assess its own generated content. By using these tokens, Self-RAG can provide a more comprehensive evaluation of answers by incorporating self-reflection into the process. This allows for better alignment with human annotators' assessments and enhances the quality and factuality of LLMs through retrieval on demand. \n",
      " SOURCE: Local Academic PDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 29 prefix-match hit, remaining 847 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16934.47 ms\n",
      "llama_perf_context_print: prompt eval time =   24971.34 ms /   847 tokens (   29.48 ms per token,    33.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     408.06 ms /     6 runs   (   68.01 ms per token,    14.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   25381.49 ms /   853 tokens\n",
      "llama_perf_context_print:    graphs reused =         83\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_21228\\1398812118.py:5: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " AGENT: Knowledge not found in local PDFs. Searching the web for: Can you give me a recipe for making a traditional Italian pizza?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 894 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   16934.47 ms\n",
      "llama_perf_context_print: prompt eval time =   26314.37 ms /   894 tokens (   29.43 ms per token,    33.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43395.12 ms /   625 runs   (   69.43 ms per token,    14.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   70082.32 ms /  1519 tokens\n",
      "llama_perf_context_print:    graphs reused =        688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANALYSIS 5 \n",
      " ANSWER: A traditional Italian pizza recipe typically includes the following ingredients and steps:\n",
      "\n",
      "Ingredients:\n",
      "- 2 cups all-purpose flour (plus extra for dusting)\n",
      "- 1/2 teaspoon salt\n",
      "- 3/4 cup warm water\n",
      "- 1 tablespoon olive oil\n",
      "- 1/2 teaspoon sugar\n",
      "- 1 packet of active dry yeast (about 2 and 1/4 teaspoons)\n",
      "- 1/2 cup tomato sauce\n",
      "- 8 ounces fresh mozzarella cheese, sliced or shredded\n",
      "- Fresh basil leaves\n",
      "- Salt to taste\n",
      "- Olive oil for brushing the pizza crust\n",
      "\n",
      "Steps:\n",
      "1. In a small bowl, dissolve sugar in warm water and sprinkle yeast over it. Let it sit until the mixture becomes frothy (about 5 minutes).\n",
      "2. In a large mixing bowl, combine flour and salt. Make a well in the center and pour in the yeast-water mixture along with olive oil. Mix everything together to form a dough.\n",
      "3. Knead the dough on a lightly floured surface for about 5 minutes until it becomes smooth and elastic. Add more flour if necessary, but be careful not to overwork the dough.\n",
      "4. Place the dough in an oiled bowl, cover with a clean kitchen towel, and let it rise in a warm place for at least 1 hour or until doubled in size.\n",
      "5. Preheat your oven to its highest setting (around 475-500°F) and prepare a pizza stone by placing it on the middle rack of the oven.\n",
      "6. Punch down the risen dough, divide it into two equal portions, and roll each portion out into a thin round shape using a rolling pin or your hands.\n",
      "7. Transfer one rolled-out dough onto a pizza peel or an inverted baking sheet sprinkled with flour to prevent sticking. Spread tomato sauce evenly over the surface of the dough, leaving a small border around the edges.\n",
      "8. Distribute slices of mozzarelö cheese on top of the sauce and add salt according to taste.\n",
      "9. Transfer the pizza onto the preheated stone in the oven using the peel or baking sheet. Bake for about 10-12 minutes, until the crust is golden brown and the cheese has melted.\n",
      "10. Remove from the oven, immediately top with fresh basil leaves, drizzle a little olive oil over it, and let it cool slightly before slicing and serving.\n",
      "\n",
      "Enjoy your homemade traditional Italian pizza! \n",
      " SOURCE: Web Search (Live Data)\n",
      "\n",
      " FINAL PERFORMANCE EVALUATION \n",
      "Total Queries: 12 | PDF Hits: 8 | Web Hits: 4 | Latency: 94.99s\n"
     ]
    }
   ],
   "source": [
    "def ask_agentic_rag(query):\n",
    "    start_time = time.time()\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    context_text = \"\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    decision_prompt = f\"\"\"<|system|> Use the context below to answer. If not found, respond ONLY with: SEARCH_WEB <|end|>\n",
    "    <|user|> Context: {context_text} \\n Question: {query} <|end|>\n",
    "    <|assistant|>\"\"\"\n",
    "    \n",
    "    decision = llm.invoke(decision_prompt).strip()\n",
    "    \n",
    "    if \"SEARCH_WEB\" in decision.upper() or not decision:\n",
    "        web_info = web_search_tool(query)\n",
    "        final_prompt = f\"<|system|> Extract answer from web info. <|end|> <|user|> Web: {web_info} \\n Q: {query} <|end|> <|assistant|>\"\n",
    "        response = llm.invoke(final_prompt).strip()\n",
    "        source = \"Web Search (Live Data)\"\n",
    "        perf_stats[\"web_hits\"] += 1\n",
    "    else:\n",
    "        response = decision\n",
    "        source = \"Local Academic PDFs\"\n",
    "        perf_stats[\"pdf_hits\"] += 1\n",
    "    \n",
    "    perf_stats[\"total_time\"] += (time.time() - start_time)\n",
    "    perf_stats[\"total_queries\"] += 1\n",
    "    return response, source\n",
    "\n",
    "# Run the test loop\n",
    "print(\"AGENTIC RAG SYSTEM READY (LOCAL + WEB)\")\n",
    "for i in range(1, 6):\n",
    "    user_query = input(f\"\\n[{i}/5] Enter Question: \")\n",
    "    if not user_query.strip(): continue\n",
    "    ans, ref = ask_agentic_rag(user_query)\n",
    "    print(f\"\\n ANALYSIS {i} \\n ANSWER: {ans} \\n SOURCE: {ref}\")\n",
    "\n",
    "# Display Stats\n",
    "avg_latency = perf_stats[\"total_time\"] / perf_stats[\"total_queries\"] if perf_stats[\"total_queries\"] > 0 else 0\n",
    "print(\"\\n FINAL PERFORMANCE EVALUATION \")\n",
    "print(f\"Total Queries: {perf_stats['total_queries']} | PDF Hits: {perf_stats['pdf_hits']} | Web Hits: {perf_stats['web_hits']} | Latency: {avg_latency:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
